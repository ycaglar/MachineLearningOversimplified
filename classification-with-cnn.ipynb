{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b335904",
   "metadata": {},
   "source": [
    "## Classification With Convolational Neural Network\n",
    "\n",
    "DNN's perform better on image classification problems if they are reinforced with convolational layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c6a6ea",
   "metadata": {},
   "source": [
    "### Boilerplate\n",
    "Install and load necessary components for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install and import TensorFlow Library\n",
    "!pip3 install Tensorflow\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "#Load Fashion MNIST Dataset\n",
    "mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c439e99",
   "metadata": {},
   "source": [
    "### Reload and re-normalize the data for CNN\n",
    "Color images have 3 channels (Red, Green, Blue) while black and white images only 1 channel. Below we convert color images to black and white images by reshaping them.\\\n",
    "Reshape function expects 4 parameters:\\\n",
    "`number of samples (len of the array), rows, cols, channels`\\\n",
    "While hepful in this case, reshaping is not alywas needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the fashion_mnist dataset\n",
    "(training_images, training_labels), (validation_images, validation_labels) = mnist.load_data()\n",
    "# here we convert 3 channel images to 1 channel images\n",
    "training_images = training_images.reshape(len(training_images), 28, 28, 1)\n",
    "validation_images = validation_images.reshape(len(validation_images), 28, 28, 1)\n",
    "# scale images\n",
    "training_images = training_images / 255.0\n",
    "validation_images = validation_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e3f010",
   "metadata": {},
   "source": [
    "### Add convolational layers and pooling layers to the model\n",
    "We will add two 2D(because images in the dataset are 2D) convolational layers each followed by a pooling layer before we get to the DNN part of the model. Convolational layers will create copies of the image and apply filters in order to extract different features such as corners, vertical lines etc by multiplying each pixel value by the filter matrix. After that, max pool layer will pick the max value within a 2 by 2 window of pixels omitting others, this will reduce the size while enhancing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b866b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  # (filter count(guesstimation), filter size(always odd numbers), ...)\n",
    "  tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu', input_shape = (28,28,1)),\n",
    "  # There are also min and avg pooling layers available\n",
    "  tf.keras.layers.MaxPooling2D(pool_size = (2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'),\n",
    "  tf.keras.layers.MaxPooling2D((2,2)),\n",
    "  # Back to the DNN part of the model\n",
    "  tf.keras.layers.Flatten(),\n",
    "  # Because now we have the convolational and pooling layers, \n",
    "  # we won't need as many neurons and layers within the DNN part of the model\n",
    "  tf.keras.layers.Dense(units = 20, activation = 'relu'),\n",
    "  tf.keras.layers.Dense(units = 10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "  optimizer = 'adam',\n",
    "  loss = 'sparse_categorical_crossentropy',\n",
    "  metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# store training history in a variable in order to plot afterwards\n",
    "history = model.fit(\n",
    "  training_images,\n",
    "  training_labels,\n",
    "  validation_data = (validation_images, validation_labels),\n",
    "  epochs = 7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda077e",
   "metadata": {},
   "source": [
    "### Utilize graphs for examining the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96225871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['validation', 'test'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
